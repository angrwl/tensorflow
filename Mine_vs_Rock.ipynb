{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import shuffle \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef read_dataset():\n    df = pd.read_csv(\"sonar.all-data.csv\")\n    print(\"Nbr columns: \",len(df.columns))\n    X = df[df.columns[0:60]].values       #all my input input a matrix except the full row which needs to be fixed\n    y = df[df.columns[60]]               #my desired output\n    \n    '''https://medium.com/@contactsunny/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621'''\n    '''This shows the difference between LabelEncoder and one_hot_encoder'''\n\n    #Encode the y i.e. convert text into numbers for easy use \n    encoder = LabelEncoder()\n    y = encoder.fit_transform(y)   #outputs 0 or 1 for mine and rock respectively i.e starts with mine (alphabetically) and give it a \n    Y = one_hot_encode(y)       #value 0 and then gives rock a value 1                       \n    print(\"X.shape\",X.shape)\n    return (X,Y)\n\n# Define the encoder function M => 1, R => 0\ndef one_hot_encode(labels):       \n    '''Creates a column for each output (in this case M or R) and then gives each column 0 or 1 depending on which column\n    we are looking at. For example if we are looking at first column (i.e. M) then each value of y which has M become 1 in \n    this column and if its R becomes 0. But in the second column of R, each M becomes 0 and R becomes 1. There is a distinction \n    between LabelEncoder() and one_hot_encode() which I need to understand'''\n    n_labels = len(labels)\n    n_unique_labels = len(np.unique(labels))\n    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n    one_hot_encode[np.arange(n_labels),labels] = 1\n    return one_hot_encode\n\nX,Y = read_dataset()\nX,Y = shuffle(X,Y)\n\n# Convert the dataset into train and test datasets and splits it up\n'''test_size = 0.20 means that it takes 20% of all the data in csv file is gonna be my test data and 80% is going to be \nused to be to infact train the data'''\ntrain_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=415)\n\n# Inspect the shape of the train and test datasets\nprint(\"train_x.shape\",train_x.shape)\nprint(\"train_y.shape\",train_y.shape)\nprint(\"test_x.shape\",test_x.shape)\nprint(\"test_y.shape\",test_y.shape)\n      \ncost_history = np.empty(shape=[1], dtype=float)\nprint(cost_history)\n",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Nbr columns:  61\nX.shape (207, 60)\ntrain_x.shape (165, 60)\ntrain_y.shape (165, 2)\ntest_x.shape (42, 60)\ntest_y.shape (42, 2)\n[5.14e-322]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"sonar.all-data.csv\")\nprint(\"Nbr columns: \",len(df.columns))\nX = df[df.columns[0:60]].values   \ny = df[df.columns[60]]       \n\ny = encoder.fit_transform(y)\nprint(y)\nn_label = len(y) \nn_label_unique = len(np.unique(y))\none_hot_encode = np.zeros((n_label,n_label_unique))\nfor j in range(n_label_unique):\n    for i in range(n_label):\n        \nprint(one_hot_encode)",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Nbr columns:  61\n[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n[[0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [0. 1.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}